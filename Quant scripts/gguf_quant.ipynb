{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "* `MODEL_ID`: The ID of the model to quantize (e.g., `mlabonne/EvolCodeLlama-7b`).\n",
    "* `QUANTIZATION_METHOD`: The quantization method to use.\n",
    "\n",
    "## Quantization methods\n",
    "\n",
    "* `q2_k`: Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\n",
    "* `q3_k_l`: Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\n",
    "* `q3_k_m`: Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\n",
    "* `q3_k_s`: Uses Q3_K for all tensors\n",
    "* `q4_0`: Original quant method, 4-bit.\n",
    "* `q4_1`: Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\n",
    "* `q4_k_m`: Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\n",
    "* `q4_k_s`: Uses Q4_K for all tensors\n",
    "* `q5_0`: Higher accuracy, higher resource usage and slower inference.\n",
    "* `q5_1`: Even higher accuracy, resource usage and slower inference.\n",
    "* `q5_k_m`: Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\n",
    "* `q5_k_s`:  Uses Q5_K for all tensors\n",
    "* `q6_k`: Uses Q8_K for all tensors\n",
    "* `q8_0`: Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.\n",
    "\n",
    "As a rule of thumb, **I recommend using Q5_K_M** as it preserves most of the model's performance. Alternatively, you can use Q4_K_M if you want to save some memory. In general, K_M versions are better than K_S versions. I cannot recommend Q2_K or Q3_* versions, as they drastically decrease model performance. T4 GPUs (Free on Colab) can handle Q5_K_M models, up to Q6_K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install llama.cpp\n",
    "!git clone https://github.com/ggerganov/llama.cpp\n",
    "!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n",
    "!pip install -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "MODEL_ID = \"R136a1/Lengkuas\"\n",
    "QUANTIZATION_METHODS = [\"q4_k_m\", \"q5_k_m\"]\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
    "\n",
    "# Download the model\n",
    "%cd /content/\n",
    "!git clone https://github.com/TheBlokeAI/AIScripts.git\n",
    "!cd AIScripts ;python hub_download.py --symlinks false $MODEL_ID \"/content/\"$MODEL_NAME\n",
    "\n",
    "# Convert to fp16\n",
    "fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n",
    "!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}\n",
    "\n",
    "# Quantize the model for each method in the QUANTIZATION_METHODS list\n",
    "for method in QUANTIZATION_METHODS:\n",
    "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n",
    "    !./llama.cpp/quantize {fp16} {qtype} {method}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title test inference\n",
    "# !wget https://huggingface.co/datasets/royallab/PIPPA-cleaned/resolve/main/pippa_raw_fix.parquet\n",
    "# !./llama.cpp/perplexity -ngl 100 -m /content/Lengkuas/lengkuas.Q5_K_M.gguf -f /content/pippa_raw_fix.parquet\n",
    "!./llama.cpp/main -ngl 100 -m /content/Lengkuas/lengkuas.Q5_K_M.gguf --prompt \"Once upon a time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Upload to HuggingFace\n",
    "!cd AIScripts ;python hub_upload.py --branch gguf \"R136a1/Madang\" \"/content/Lengkuas/lengkuas.Q6_K.gguf\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
