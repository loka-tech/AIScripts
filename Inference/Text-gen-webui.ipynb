{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JUST RUN THIS ON GOOGLE COLAB, OKAY? AND MAKE SURE TO USE GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. clone the repo and install all the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install TGWUI\n",
    "silent = True # @param {type:\"boolean\"}\n",
    "logins = True # @param {type:\"boolean\"}\n",
    "Model = \"\"\n",
    "%cd /content/\n",
    "if silent:\n",
    "  !cd /content/ ; git clone -q https://github.com/TheBlokeAI/AIScripts.git -q\n",
    "  !pip install GPUtil -q -q -q --progress-bar off > /dev/null\n",
    "  !pip install huggingface_hub -q -q -q --progress-bar off > /dev/null\n",
    "  !pip install hf_transfer -q -q -q --progress-bar off > /dev/null\n",
    "  if logins:\n",
    "    from huggingface_hub import login\n",
    "    login(\"ur hf key here\", add_to_git_credential=True)\n",
    "  import torch\n",
    "  from pathlib import Path\n",
    "  import time\n",
    "  import threading\n",
    "  import GPUtil\n",
    "  from tabulate import tabulate\n",
    "  !wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb -q -nv\n",
    "  !DEBIAN_FRONTEND=noninteractive apt-get install -qq ./cloudflared-linux-amd64.deb aria2 < /dev/null > /dev/null\n",
    "  !rm cloudflared-linux-amd64.deb\n",
    "\n",
    "  !rm -r /content/sample_data\n",
    "  if Path.cwd().name != 'text-generation-webui':\n",
    "    print('\\033[1;32;1m\\nInstalling...\\n\\033[0;37;0m')\n",
    "\n",
    "    !git clone -q -b dev https://github.com/oobabooga/text-generation-webui\n",
    "    !ln -s text-generation-webui/models .\n",
    "    %cd text-generation-webui\n",
    "\n",
    "    torver = torch.__version__\n",
    "    print(f\"TORCH: {torver}\")\n",
    "    is_cuda118 = '+cu118' in torver  # 2.1.0+cu118\n",
    "    is_cuda117 = '+cu117' in torver  # 2.0.1+cu117\n",
    "\n",
    "    textgen_requirements = open('requirements.txt').read().splitlines()\n",
    "    if is_cuda117:\n",
    "        textgen_requirements = [req.replace('+cu121', '+cu117').replace('+cu122', '+cu117').replace('torch2.1', 'torch2.0') for req in textgen_requirements]\n",
    "    elif is_cuda118:\n",
    "        textgen_requirements = [req.replace('+cu121', '+cu118').replace('+cu122', '+cu118') for req in textgen_requirements]\n",
    "    with open('temp_requirements.txt', 'w') as file:\n",
    "        file.write('\\n'.join(textgen_requirements))\n",
    "\n",
    "    !pip install -r extensions/openai/requirements.txt --upgrade -q -q -q --progress-bar off --no-warn-conflicts > /dev/null\n",
    "    !pip install -r temp_requirements.txt --upgrade -q -q -q --progress-bar off --no-warn-conflicts > /dev/null\n",
    "\n",
    "    try:\n",
    "      import flash_attn\n",
    "    except:\n",
    "      !pip uninstall -y flash_attn\n",
    "\n",
    "  print('\\033[1;32;1m\\nDone!\\n\\033[0;37;0m')\n",
    "else:\n",
    "  !cd /content/ ; git clone -q https://github.com/TheBlokeAI/AIScripts.git\n",
    "  !pip install GPUtil\n",
    "  !pip install huggingface_hub\n",
    "  !pip install hf_transfer\n",
    "  if logins:\n",
    "    from huggingface_hub import login\n",
    "    login(\"ur hf key here\", add_to_git_credential=True)\n",
    "  import torch\n",
    "  from pathlib import Path\n",
    "  import time\n",
    "  import threading\n",
    "  import GPUtil\n",
    "  from tabulate import tabulate\n",
    "  !wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb -q -nv\n",
    "  !apt install ./cloudflared-linux-amd64.deb aria2\n",
    "  !rm cloudflared-linux-amd64.deb\n",
    "\n",
    "  !rm -r /content/sample_data\n",
    "  if Path.cwd().name != 'text-generation-webui':\n",
    "    print('\\033[1;32;1m\\nInstalling...\\n\\033[0;37;0m')\n",
    "\n",
    "    !git clone -b dev https://github.com/oobabooga/text-generation-webui\n",
    "    !ln -s text-generation-webui/models .\n",
    "    %cd text-generation-webui\n",
    "\n",
    "    torver = torch.__version__\n",
    "    print(f\"TORCH: {torver}\")\n",
    "    is_cuda118 = '+cu118' in torver  # 2.1.0+cu118\n",
    "    is_cuda117 = '+cu117' in torver  # 2.0.1+cu117\n",
    "\n",
    "    textgen_requirements = open('requirements.txt').read().splitlines()\n",
    "    if is_cuda117:\n",
    "        textgen_requirements = [req.replace('+cu121', '+cu117').replace('+cu122', '+cu117').replace('torch2.1', 'torch2.0') for req in textgen_requirements]\n",
    "    elif is_cuda118:\n",
    "        textgen_requirements = [req.replace('+cu121', '+cu118').replace('+cu122', '+cu118') for req in textgen_requirements]\n",
    "    with open('temp_requirements.txt', 'w') as file:\n",
    "        file.write('\\n'.join(textgen_requirements))\n",
    "\n",
    "    !pip install -r extensions/openai/requirements.txt --upgrade --no-warn-conflicts\n",
    "    !pip install -r temp_requirements.txt --upgrade --no-warn-conflicts\n",
    "\n",
    "    try:\n",
    "      import flash_attn\n",
    "    except:\n",
    "      !pip uninstall -y flash_attn\n",
    "\n",
    "  print('\\033[1;32;1m\\nDone!\\n\\033[0;37;0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download da model :>\n",
    "\n",
    "download model sendiri, pilih di https://huggingface.co/models (hanya untuk text generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Model\n",
    "#@markdown If unsure about the branch, write \"main\" or leave it blank.\n",
    "\n",
    "branch = \"main\" # @param [\"main\", \"6bit\", \"gptq-4bit-32g-actorder_True\", \"gptq-4bit-128g-actorder_True\"] {allow-input: true}\n",
    "\n",
    "if branch == \"\":\n",
    "  branch = \"main\"\n",
    "\n",
    "Models = \"R136a1/TimeCrystal-l2-13B-exl2\" # @param [\"R136a1/MythoMax-L2-13B-exl2\", \"TheBloke/Euryale-1.3-L2-70B-GPTQ\", \"TheBloke/Xwin-LM-70B-V0.1-GPTQ\", \"R136a1/l2-13b-thespurral-v1-exl2\", \"R136a1/TimeCrystal-l2-13B-exl2\", \"R136a1/Noromaid-20b-v0.1.1-exl2\", \"R136a1/TimeLess-20B-exl2\", \"R136a1/TimeMax-20B-exl2\"] {allow-input: true}\n",
    "Model = Models.split(\"/\")[-1]\n",
    "\n",
    "!pip show exllamav2\n",
    "\n",
    "%cd /content/AIScripts/\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1\n",
    "\n",
    "!python hub_download.py --symlinks false $Models \"/content/models/\"$Model --branch $branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gass ga si? gass lah (kalau tidak punya tunnel cloudflarenya, pakai mode \"no tunnel + no model\")\n",
    "\n",
    "\"no model\" artinya tidak ada model yang di load, jadi load sendiri dari gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title GASS\n",
    "Token = \"isi pake Tunnel ID sendiri\" #@param {type:\"string\"}\n",
    "Loader = \"Exllamav2_hf\" # @param [\"Exllamav2_hf\", \"Exllamav2\"]\n",
    "mode = \"no model\" # @param [\"gas\", \"no model\", \"no tunnel + no model\"]\n",
    "\n",
    "# monitor GPU stats\n",
    "def gpuinfo():\n",
    "    i = 0\n",
    "    while True:\n",
    "        # GPU information\n",
    "        print(\"=\"*40, \"GPU Details\", \"=\"*40)\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        list_gpus = []\n",
    "        for gpu in gpus:\n",
    "            # get the GPU id\n",
    "            gpu_id = gpu.id\n",
    "            # name of GPU\n",
    "            gpu_name = gpu.name\n",
    "            # get % percentage of GPU usage of that GPU\n",
    "            gpu_load = f\"{gpu.load*100}%\"\n",
    "            # get free memory in MB format\n",
    "            gpu_free_memory = f\"{gpu.memoryFree}MB\"\n",
    "            # get used memory\n",
    "            gpu_used_memory = f\"{gpu.memoryUsed}MB\"\n",
    "            # get total memory\n",
    "            gpu_total_memory = f\"{gpu.memoryTotal}MB\"\n",
    "            # get GPU temperature in Celsius\n",
    "            gpu_temperature = f\"{gpu.temperature} Â°C\"\n",
    "            gpu_uuid = gpu.uuid\n",
    "            list_gpus.append((\n",
    "                gpu_id, gpu_name, gpu_load, gpu_free_memory, gpu_used_memory,\n",
    "                gpu_total_memory, gpu_temperature, gpu_uuid\n",
    "            ))\n",
    "\n",
    "        print(tabulate(list_gpus, headers=(\"id\", \"name\", \"load\", \"free memory\", \"used memory\", \"total memory\",\n",
    "                                        \"temperature\", \"uuid\")))\n",
    "        print(\"\\n\"*2)\n",
    "        time.sleep(600)\n",
    "thread1 = threading.Thread(target=gpuinfo)\n",
    "thread1.start()\n",
    "\n",
    "server1 = f\"cd /content/text-generation-webui ;python server.py --extensions openai --gradio-auth user:admin\"\n",
    "server2 = f\"cd /content/text-generation-webui ;python server.py --extensions openai --gradio-auth user:admin --public-api --share\"\n",
    "server = f\"cd /content/text-generation-webui ;python server.py --model {Model} --extensions openai --loader {Loader} --max_seq_len 4096 --compress_pos_emb 1 --gradio-auth user:admin\"\n",
    "if mode == \"no tunnel + no model\":\n",
    "  !$server2\n",
    "if mode == \"no model\":\n",
    "  !cloudflared service install $Token\n",
    "  !$server1\n",
    "else:\n",
    "  !!cloudflared service install $Token\n",
    "  !$server"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
